---
date: 2026-02-06
tags:
  - 强化学习
---
![[Pasted image 20260207014946.png]]

# 强化学习基础 (Lecture 1-4) 复习指南

## 🗺️ 知识地图：我们学了什么？

这四章构建了强化学习最核心的“**表格型方法 (Tabular Methods)**”理论大厦：
1. **Lecture 1 (建模)**: 将现实问题抽象为 **MDP**（状态、动作、奖励、策略）。
2. **Lecture 2 (评估)**: 给定一个策略，如何评价它好不好？ $\rightarrow$ **贝尔曼方程** ($v_\pi$)。
3. **Lecture 3 (目标)**: 什么样的策略才是最好的？ $\rightarrow$ **贝尔曼最优方程** ($v^*$)。
4. **Lecture 4 (求解)**: 如何用算法找到这个最好的策略？ $\rightarrow$ **值迭代 & 策略迭代**。
    

---

## 📝 核心内容回顾
![[L1-4总结图.pdf]]

![[Pasted image 20260207014836.png]]


### 第一部分：基本组件 (Lecture 1)
**核心任务**：建立语言系统。
- **MDP五元组**：$\mathcal{S}$ (状态), $\mathcal{A}$ (动作), $\mathcal{R}$ (奖励), $p(s'|s,a)$ (转移概率), $\gamma$ (折扣因子)。
- **两个概率**：
    - 策略 $\pi(a|s)$：Agent控制的概率。
    - 转移 $p(s'|s,a)$：环境控制的客观规律。
- **一个假设**：**马尔可夫性质 (Markov Property)** —— 未来只取决于现在，与过去无关。
- **一个目标**：最大化 **折扣回报 (Discounted Return)** $G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$。
    
### 第二部分：策略评估 (Lecture 2)

**核心任务**：求“期望”。已知策略 $\pi$，算它值多少分。
- **状态价值 $v_\pi(s)$**：站在状态 $s$，眼前遵循策略 $\pi$，未来的累积回报期望。
- **贝尔曼方程 (The Bellman Equation)**：
    $$v_{\pi}(s) = \sum_{a} \pi(a|s) \left[ \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v_{\pi}(s') \right]$$
    - _直觉_：当前的价值 = 平均即时奖励 + 打折后的平均未来价值。
    - _性质_：这是一个**线性方程组**。
    - _矩阵形式_：$v_\pi = r_\pi + \gamma P_\pi v_\pi$。
        
### 第三部分：最优控制 (Lecture 3)

**核心任务**：求“最大化”。寻找那个让价值最大的策略 $\pi^*$。
- **最优价值 $v^*(s)$**：所有可能策略中，能达到的最大 $v(s)$。
- **贝尔曼最优方程 (BOE)**：
    $$v^*(s) = \max_{a} \left[ \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v^*(s') \right]$$
    
    - _区别_：方程中出现了 **$\max$** 操作，使其变为**非线性方程**。
    - _理论保障_：**压缩映射定理**。保证了 $v^*$ 存在、唯一，且可以通过迭代收敛。
- **重要结论**：总存在一个**确定性**的最优策略。
    

### 第四部分：动态规划算法 (Lecture 4)

**核心任务**：把数学方程转化为代码算法。
- **值迭代 (Value Iteration, VI)**：
    - 直接迭代 BOE 公式。
    - **特点**：每步更新都在取 max（不仅看路，还换方向）。收敛较慢，但每步计算量小。
    - _公式_：$v_{k+1} = \max(r + \gamma P v_k)$
        
- **策略迭代 (Policy Iteration, PI)**：
    - 分为两步：**策略评估 (PE)** (解方程算出 $v_\pi$) + **策略改进 (PI)** (贪婪更新 $\pi$)。
    - **特点**：收敛极快（步数少），但每一步内部要做很多运算（PE阶段）。
- **截断策略迭代**：PI 的“省流版”，PE 阶段不求精确解，只迭代几步就进行改进。
    



---

## ⚖️ 核心概念对比 (易混淆点)

|**维度**|**贝尔曼方程 (Bellman Eq)**|**贝尔曼最优方程 (Bellman Optimality Eq)**|
|---|---|---|
|**针对对象**|针对一个**固定**的策略 $\pi$|针对**最优**策略 $\pi^*$|
|**数学操作**|期望 (Expectation, $\sum \pi \dots$)|最大化 (Maximization, $\max_a \dots$)|
|**方程类型**|线性方程组 (Linear)|非线性方程组 (Non-linear)|
|**求解难度**|容易 (可直接求逆或简单迭代)|较难 (必须迭代求解，无法直接求逆)|
|**对应算法**|策略评估 (Policy Evaluation)|值迭代 (Value Iteration)|

|**维度**|**值迭代 (VI)**|**策略迭代 (PI)**|
|---|---|---|
|**核心逻辑**|主要是更新值 $v$，最后提取 $\pi$|显式地在 $\pi \to v \to \pi$ 之间循环|
|**每步操作**|$v \leftarrow \max(r+\gamma P v)$|1. 解方程 $v=r+\gamma P v$<br><br>  <br><br>2. $\pi \leftarrow \arg\max(q)$|
|**效率**|迭代步数多，单步快|迭代步数少，单步慢|

---

## 🧠 自检题 (Self-Check Quiz)

请尝试不看上面的内容回答以下问题，以检验掌握程度。

### 第一关：概念辨析 (True/False)

1. [ ] 在马尔可夫决策过程 (MDP) 中，下一个状态 $S_{t+1}$ 既取决于当前状态 $S_t$ 和动作 $A_t$，也取决于过去的状态 $S_{t-1}$。
2. [ ] 折扣因子 $\gamma = 0$ 意味着智能体 (Agent) 非常短视，只关心当前的即时奖励。
3. [ ] 对于同一个 MDP 环境，最优策略 $\pi^*$ 只有唯一的一个。
4. [ ] 贝尔曼方程（非最优）是一个线性方程，可以直接用矩阵求逆 $v = (I - \gamma P)^{-1}r$ 求解。
5. [ ] 如果我们把环境中的所有奖励 $r$ 都乘以 10 并加上 5，最优策略 $\pi^*$ 会发生改变。
    
### 第二关：核心理解 (Short Answer)

6. **关于 Return**: 为什么在处理连续性任务 (Continuing Tasks) 时，必须引入折扣因子 $\gamma$？
7. **关于 BOE**: 贝尔曼最优方程右边的 $\max_a$ 操作是在做什么？它对应了强化学习中的什么核心目的？
8. **算法对比**: 在策略迭代 (Policy Iteration) 中，如果我们在“策略评估 (Policy Evaluation)”阶段不等到 $v_\pi$ 完全收敛，而是只迭代 3 次就去更新策略，这叫什么算法？
9. **数学直觉**: $v_\pi(s)$ 和 $q_\pi(s,a)$ 有什么关系？请用一句话描述（或写出公式）。
    

---

### ✅ 答案与解析

**第一关答案：**

1. **False**。MDP 满足马尔可夫性质，未来只取决于当前 ($S_t, A_t$)，与历史无关。
2. **True**。$\gamma=0$ 导致后续项全为 0，$G_t = R_{t+1}$。
3. **False**。**最优价值函数 $v^*$ 是唯一的**，但达到这个价值的**最优策略 $\pi^*$ 可能不唯一**（例如两个动作都能带来同样的最大收益）。
4. **True**。这是 Lecture 2 的核心结论之一。
5. **False**。根据 Lecture 3 的**仿射变换不变性**，线性变换不会改变数值的相对大小顺序，因此 $\arg\max$ 选出的动作不变。

**第二关答案：**

6. **为了数学收敛性**。如果没有 $\gamma$ (即 $\gamma=1$) 且任务无限进行，回报的总和可能会发散趋于无穷大，导致无法比较策略优劣。
7. **贪婪选择 (Greedy Selection)**。它代表在当前状态下，选择那个能让（即时奖励+未来潜在价值）最大的动作。它对应了强化学习中的**控制 (Control)** 或 **优化 (Optimization)** 目的。
8. **截断策略迭代 (Truncated Policy Iteration)**。这也是值迭代 (迭代次数=1) 和策略迭代 (迭代次数=$\infty$) 的中间形态。
9. **$v$ 是 $q$ 的期望**。状态价值等于该状态下所有动作价值 $q_\pi(s,a)$ 关于策略概率 $\pi(a|s)$ 的加权平均。
* 公式：$v_\pi(s) = \sum_a \pi(a|s) q_\pi(s,a)$。