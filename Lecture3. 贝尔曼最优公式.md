---
date: 2026-01-29
tags:
  - 强化学习
---
# 第三章：贝尔曼最优方程 (Bellman Optimality Equation)

> [!info] 章节导读
> 在[[Lecture2. 贝尔曼公式|上一章]]中，我们学习了**贝尔曼方程 (Bellman Equation)**，它是用于评估一个**给定策略 (Fixed Policy)** 的工具。但在强化学习中，我们的终极目标不仅仅是评估，而是要找到**最优策略 (Optimal Policy)**。
>
> 本章的核心任务是建立描述最优策略和最优价值函数的数学方程——**贝尔曼最优方程 (Bellman Optimality Equation, BOE)**，并探讨如何求解它。这是从“策略评估”走向“策略控制”的关键一步。

---

## 1. 最优策略的定义 (Definition of Optimal Policy)

在寻找最优策略之前，我们必须先定义什么叫“更好”。在强化学习中，策略的优劣是通过**状态价值 (State Value)** 来判定的。

### 1.1 策略的偏序关系

> [!abstract] 定义：策略的优劣
> 如果对于状态空间 $\mathcal{S}$ 中的**所有**状态 $s$，策略 $\pi_1$ 下的状态价值都大于或等于策略 $\pi_2$ 下的状态价值，即：
> $$v_{\pi_1}(s) \ge v_{\pi_2}(s), \quad \forall s \in \mathcal{S}$$
> 那么我们称策略 $\pi_1$ **优于** (better than) 策略 $\pi_2$。

### 1.2 最优策略的定义

基于上述偏序关系，我们可以定义**最优策略 (Optimal Policy)**。

> [!abstract] 定义：最优策略
> 一个策略 $\pi^*$ 被称为最优策略，如果它优于或等价于所有其他可能的策略 $\pi$：
> $$v_{\pi^*}(s) \ge v_{\pi}(s), \quad \forall s \in \mathcal{S}, \forall \pi$$
>

**随之而来的关键问题：**
1.  **存在性 (Existence)**：最优策略一定存在吗？
2.  **唯一性 (Uniqueness)**：最优策略是唯一的吗？（通常策略本身不唯一，但最优价值函数是唯一的）
3.  **确定性 (Stochastic vs Deterministic)**：最优策略是确定性的还是随机性的？
4.  **求解 (Algorithm)**：如何找到这个策略？

解答这些问题的钥匙，就是**贝尔曼最优方程 (BOE)**。

---

## 2. 贝尔曼最优方程 (BOE) 的推导

### 2.1 从贝尔曼方程到贝尔曼最优方程

回顾**贝尔曼方程**（针对特定策略 $\pi$）：
$$v_{\pi}(s) = \sum_{a} \pi(a|s) \left[ \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v_{\pi}(s') \right]$$

为了得到最优状态价值 $v^*(s)$，我们需要在所有可能的策略中寻找使价值最大化的那个。因此，**贝尔曼最优方程**可以写为：

> [!math] 贝尔曼最优方程 (Elementwise Form)
> $$v(s) = \max_{\pi} \sum_{a} \pi(a|s) \underbrace{\left( \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v(s') \right)}_{q(s,a)}$$
> 其中 $q(s,a)$ 是在状态 $s$ 采取动作 $a$ 对应的动作价值。

### 2.2 简化右侧的 $\max_{\pi}$ (Maximization on RHS)

这个方程的右边看起来很复杂：$\max_{\pi} \sum_{a} \pi(a|s) q(s,a)$。我们需要解这个最大化问题。

> [!example] 数学直觉
> 假设要在状态 $s$ 选择策略 $\pi(\cdot|s)$ 来最大化 $\sum_{a} \pi(a|s) q(s,a)$。
> 已知约束条件是 $\sum_{a} \pi(a|s) = 1$ 且 $\pi(a|s) \ge 0$。
>
> 这实际上是在求一组加权系数，使得加权和最大。显然，**将所有的权重（概率）都分配给那个数值最大的 $q(s,a)$**，也就是 $a^* = \arg\max_a q(s,a)$，能得到最大值。

因此，我们可以得出重要结论：
$$\max_{\pi} \sum_{a} \pi(a|s) q(s,a) = \max_{a} q(s,a)$$

**BOE 的最终形式：**

> [!math] 贝尔曼最优方程 (最终形式)
> $$v(s) = \max_{a} \left[ \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v(s') \right], \quad \forall s \in \mathcal{S}$$
>

这告诉我们：**最优状态价值等于在该状态下，选择最优动作后所能获得的期望回报。**

### 2.3 矩阵向量形式 (Matrix-Vector Form)

为了后续的理论分析，我们将其写成向量形式：
$$v = \max_{\pi} (r_{\pi} + \gamma P_{\pi} v)$$
注意：这里的 $\max_{\pi}$ 是对向量的每个元素逐个进行的。这个方程看起来既优雅又棘手：
* **优雅**：形式简洁。
* **棘手**：右侧包含非线性的 $\max$ 操作，这意味着我们不能像求解线性贝尔曼方程那样直接使用矩阵求逆 ($v = (I - \gamma P)^{-1}r$)。

---

## 3. 压缩映射定理 (Contraction Mapping Theorem)

为了证明 BOE 有解且可解，我们需要引入数学工具：**压缩映射定理**。

### 3.1 核心概念

1.  **不动点 (Fixed Point)**：对于函数 $f: X \to X$，如果 $f(x) = x$，则 $x$ 是不动点。BOE 的解 $v^*$ 就是函数 $f(v) = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v)$ 的不动点。
2.  **压缩映射 (Contraction Mapping)**：如果在某种范数 $\|\cdot\|$ 下，满足：
    $$\|f(x_1) - f(x_2)\| \le \gamma \|x_1 - x_2\|$$
    且 $0 \le \gamma < 1$，则称 $f$ 为压缩映射。

### 3.2 定理内容

> [!math] 压缩映射定理 (Contraction Mapping Theorem)
> 如果 $f$ 是一个完备度量空间上的压缩映射，那么：
> 1.  **存在性 (Existence)**：存在一个不动点 $x^*$ 使得 $f(x^*) = x^*$。
> 2.  **唯一性 (Uniqueness)**：该不动点是唯一的。
> 3.  **算法收敛性 (Algorithm)**：对于任意初始值 $x_0$，序列 $x_{k+1} = f(x_k)$ 当 $k \to \infty$ 时，以指数速度收敛于 $x^*$。

### 3.3 应用于 BOE

可以证明，BOE 定义的函数 $f(v) = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v)$ 在无穷范数 $\|\cdot\|_\infty$ 下是一个**压缩映射**，且压缩因子正是折扣因子 $\gamma$。

这意味着：
> [!important] 结论
> 贝尔曼最优方程**一定有解**，解是**唯一的**，且可以通过**迭代法**求解。

---

## 4. 求解 BOE：值迭代算法 (Value Iteration)

基于压缩映射定理，我们可以直接构造求解算法。这实际上就是著名的 **值迭代 (Value Iteration)** 算法的核心思想（虽然更详细的讨论在下一章，但这里是数学源头）。

### 4.1 迭代步骤

给定任意初始价值猜测 $v_0$（例如全为 0），我们重复以下步骤直到收敛：

1.  **计算 q-values**：对于每个状态 $s$ 和每个动作 $a$，利用上一轮的 $v_k$ 计算：
    $$q_k(s,a) = \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v_k(s')$$
   
2.  **贪婪更新 (Maximization)**：更新状态价值 $v_{k+1}(s)$ 为最大的 q-value：
    $$v_{k+1}(s) = \max_{a} q_k(s,a)$$
   

### 4.2 导出最优策略

当 $v_k$ 收敛到 $v^*$ 后，我们可以轻松获得最优策略 $\pi^*$。最优策略是**贪婪的 (Greedy)**：
$$\pi^*(a|s) = \begin{cases} 1 & a = \arg\max_{a} q^*(s,a) \\ 0 & \text{otherwise} \end{cases}$$

> [!important] 性质
> 总是存在一个**确定性 (Deterministic)** 的最优策略。我们不需要随机策略来达到最优。

---

## 5. 影响最优策略的因素

最优策略是由环境动力学、奖励函数和折扣因子共同决定的。通过 Grid World 案例，我们可以深入理解以下参数的影响：

### 5.1 折扣因子 $\gamma$ (Discount Rate)

* **大 $\gamma$ (接近 1)**：Agent 变得“远视 (Far-sighted)”。它非常在乎未来的收益。
    * *案例*：在 $\gamma=0.9$ 的案例中，Agent 甚至愿意冒着进入陷阱（-10 惩罚）的风险去寻找更远的捷径或目标，因为它计算出长期的正收益能够覆盖短期的风险。
* **小 $\gamma$ (接近 0)**：Agent 变得“近视 (Short-sighted)”。它只在乎眼前的立即奖励。
    * *案例*：在 $\gamma=0.5$ 的案例中，Agent 变得谨慎，避开所有危险区域，即使那是一条捷径。
    * *极端情况*：在 $\gamma=0$ 的案例中，Agent 只选择当前 $r$ 最大的动作，甚至可能甚至无法到达目标。

### 5.2 奖励函数设计 $r$ (Reward Design)

* **惩罚力度**：如果增加对危险区域的惩罚（例如从 -1 变 -10），最优策略会相应调整以避开这些区域。
* **仿射变换不变性 (Affine Transformation)**：
    如果对所有奖励进行变换 $r' = ar + b$ (其中 $a > 0$)，最优策略**保持不变**。
    * *原因*：新的最优价值 $v'$ 只是原价值 $v^*$ 的线性变换：$v' = av^* + \frac{b}{1-\gamma}\mathbf{1}$。比较大小时，相对顺序不变。

### 5.3 为什么不走“无意义的弯路”？

在相关的例子中，即使在这个 Grid World 中每走一步没有显式的惩罚（reward=0），最优策略也不会在原地转圈或绕远路。
* **原因**：只要 $\gamma < 1$，推迟到达目标（获取正奖励）会因为折扣因子 $\gamma^t$ 而导致总回报降低。
* *数学解释*：
    * 直达：$Return = 1 + \gamma 1 + \dots = \frac{1}{1-\gamma}$
    * 绕路：$Return = 0 + \dots + \gamma^k 1 + \dots$
    * 显然，越早获得奖励，现值越高。

---

## 6. 总结 (Summary)

本章构建了强化学习的理论基石：

1.  **最优策略**定义为在所有状态下产生最大价值的策略。
2.  **贝尔曼最优方程 (BOE)**：
    $$v(s) = \max_{a} \left[ \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v(s') \right]$$
    它利用 `max` 操作符将“寻找最优策略”的问题转化为“求解非线性方程组”的问题。
3.  **压缩映射定理**保证了 BOE 存在唯一的解，并且可以通过迭代法（$v_{k+1} = f(v_k)$）找到这个解。
4.  这个迭代求解的过程，正是**值迭代 (Value Iteration)** 算法的原型，我们将在下一章详细讨论具体的算法实现。

> [!tip] 下一步
> 现在我们有了描述最优解的方程 (BOE)，下一章 (Chapter 4) 我们将深入具体的动态规划算法：**值迭代 (Value Iteration)** 和 **策略迭代 (Policy Iteration)**，将这些数学原理转化为代码逻辑。