---
date: 2026-01-28
tags:
  - 强化学习
---
# Lecture 2: Bellman Equation (贝尔曼方程)

## 1. 核心直觉 (Intuition)

在深入数学推导之前，我们要先理解本章到底在解决什么问题。

> [!example] 生活中的类比：股票投资评估
> 想象你是一位投资者，正在评估两套不同的投资策略（**Policy, $\pi$**）：
> 1.  **策略 A**：高风险高回报，初期可能亏损，但长期期望大涨。
> 2.  **策略 B**：稳健理财，每年固定收益。
>
> 即使现在的市场状况（**State, $s$**）完全一样，你对这两个策略的“估值”也是不同的。
> * 我们需要一个数学量来衡量“在这个状态下，坚持用这套策略，未来总共能拿到多少好处”。
> * 这个量就是 **状态价值 (State Value)**。
> * 为了算出这个价值，我们不能只看眼前的收益，还要看未来的收益（并打个折）。这就引出了 **回报 (Return)** 的概念。

---

## 2. 基础定义：回报与价值 (Return & Value)

### 2.1 回报 (Return)

在一个马尔可夫奖励过程（MRP）或决策过程（MDP）中，我们关注的是长期的累积奖励。

> [!abstract] 定义：折扣回报 (Discounted Return)
> 这是一个**随机变量 (Random Variable)**，通常记为 $G_t$。它表示从 $t$ 时刻开始，沿某条轨迹（Trajectory）所获得的所有奖励的折扣和：
>
> $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
>
> * **$\gamma \in [0, 1)$**：**折扣因子 (Discount Factor)**。
>     * 数学上：保证无穷级数收敛（即回报有限）。
>     * 直觉上：体现了对未来的“不耐烦”或未来的不确定性（明天的100块不如今天的100块值钱）。
> * **注意**：$R_{t+1}, R_{t+2}, \dots$ 都是随机变量，所以 $G_t$ 也是随机的。

### 2.2 状态价值 (State Value)

因为 $G_t$ 是随机的（每次跑出来的轨迹可能不一样），我们无法直接用它来评估一个状态的好坏。我们需要取它的**期望**。

> [!abstract] 定义：状态价值函数 (State-Value Function)
> 记为 $v_{\pi}(s)$。它表示在策略 $\pi$ 下，从状态 $s$ 出发，能够获得的**期望**回报。
>
> $$v_{\pi}(s) = \mathbb{E}[G_t | S_t = s]$$
>
> * **$v_{\pi}(s)$ 是一个确定性的数值 (Deterministic Value)**，而不是随机变量。
> * 它是策略 $\pi$ 的函数。策略不同，同一个状态的价值也会变。

---

## 3. 贝尔曼方程 (The Bellman Equation)

这是本章的**灵魂**。它描述了**当前状态价值**与**未来状态价值**之间的递归关系。这使得我们不需要把无穷远的未来都跑一遍，只要知道“下一步”的信息就能算出现在的价值。

### 3.1 详细数学推导 (Step-by-Step Derivation)

我们需要从 $G_t$ 的定义出发，推导出 $v_{\pi}(s)$ 的递归形式。

> [!math] 推导过程
>
> **第一步：展开 $G_t$**
> 利用 $G_t$ 的定义，将其拆解为“即时奖励”和“剩余回报”：
> $$
> \begin{aligned}
> G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \\
> &= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \\
> &= R_{t+1} + \gamma G_{t+1} \quad \text{此处利用了递归结构}
> \end{aligned}
> $$
>
> **第二步：代入价值函数定义**
> 将上式代入 $v_{\pi}(s) = \mathbb{E}[G_t | S_t = s]$：
> $$
> v_{\pi}(s) = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s]
> $$
>
> **第三步：利用期望的线性性质 (Linearity of Expectation)**
> $$
> v_{\pi}(s) = \underbrace{\mathbb{E}[R_{t+1} | S_t = s]}_{\text{即时奖励的期望}} + \gamma \underbrace{\mathbb{E}[G_{t+1} | S_t = s]}_{\text{未来回报的期望}}
> $$
>
> **第四步：拆解各项 (Expansion)**
>
> * **项 A：即时奖励期望 $\mathbb{E}[R_{t+1} | S_t = s]$**
>     这里利用全期望公式，对动作 $a$ 和奖励 $r$ 进行求和：
>     $$\mathbb{E}[R_{t+1} | S_t = s] = \sum_{a} \pi(a|s) \sum_{r} p(r|s,a) \cdot r$$
>     *(解释：先按策略 $\pi$ 选动作 $a$，再根据环境模型 $p$ 得到奖励 $r$)*
>
> * **项 B：未来回报期望 $\mathbb{E}[G_{t+1} | S_t = s]$**
>     这里比较关键，需要引入下一时刻的状态 $S_{t+1} = s'$：
>     $$
>     \begin{aligned}
>     \mathbb{E}[G_{t+1} | S_t = s] &= \sum_{s'} \mathbb{E}[G_{t+1} | S_t=s, S_{t+1}=s'] p(s'|s) \\
>     &= \sum_{s'} \mathbb{E}[G_{t+1} | S_{t+1}=s'] p(s'|s) \quad \text{(马尔可夫性: 遗忘历史)} \\
>     &= \sum_{s'} v_{\pi}(s') p(s'|s) \quad \text{(代入状态价值定义)}
>     \end{aligned}
>     $$
>     其中 $p(s'|s)$ 是状态转移概率，展开动作 $a$ 后为：$\sum_{a} \pi(a|s) p(s'|s,a)$。
>
> **第五步：合并 (Result)**
> 将项 A 和项 B 组合，得到标准的 **Bellman Equation**：
>
> $$v_{\pi}(s) = \sum_{a} \pi(a|s) \left[ \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v_{\pi}(s') \right]$$

> [!important] 理解 Bootstrapping (自举)
> 观察公式右边，我们在计算 $v_{\pi}(s)$ 时用到了 $v_{\pi}(s')$。
> * **Bootstrapping**: 用一个估计值（$v_{\pi}(s')$）去更新另一个估计值（$v_{\pi}(s)$）。
> * 这说明所有状态的价值是**相互依赖**的，它们构成了一个方程组。

---

## 4. 贝尔曼方程的矩阵形式 (Matrix-Vector Form)

当状态空间有限（$n$ 个状态）时，我们可以把贝尔曼方程写成漂亮且强大的线性代数形式。这对计算解非常重要。

### 4.1 定义向量与矩阵
假设状态为 $s_1, \dots, s_n$。

1.  **价值向量 $v_{\pi}$**: $n \times 1$ 列向量。
    $$v_{\pi} = [v_{\pi}(s_1), \dots, v_{\pi}(s_n)]^T$$
2.  **期望即时奖励向量 $r_{\pi}$**: $n \times 1$ 列向量。
    $$r_{\pi}(s_i) = \sum_{a} \pi(a|s_i) \sum_{r} p(r|s_i, a)r$$
3.  **状态转移矩阵 $P_{\pi}$**: $n \times n$ 矩阵。
    $$[P_{\pi}]_{ij} = p(s_j | s_i) = \sum_{a} \pi(a|s_i) p(s_j | s_i, a)$$
    *(注意：矩阵的第 $i$ 行第 $j$ 列表示从 $s_i$ 转移到 $s_j$ 的概率)*

### 4.2 矩阵方程
将代数形式重写为矩阵形式：
$$v_{\pi} = r_{\pi} + \gamma P_{\pi} v_{\pi}$$

> [!math] 求解状态价值
> 这是一个形如 $x = b + Ax$ 的线性方程组。只要矩阵 $(I - \gamma P_{\pi})$ 可逆，就有解析解：
>
> $$
> \begin{aligned}
> v_{\pi} - \gamma P_{\pi} v_{\pi} &= r_{\pi} \\
> (I - \gamma P_{\pi}) v_{\pi} &= r_{\pi} \\
> v_{\pi} &= (I - \gamma P_{\pi})^{-1} r_{\pi}
> \end{aligned}
> $$
>
> * **可逆性**: 只要 $\gamma < 1$，矩阵 $(I - \gamma P_{\pi})$ 通常是可逆的。
> * **局限性**: 矩阵求逆的计算复杂度是 $O(n^3)$。对于状态极多的问题（如围棋），这种**闭式解 (Closed-form solution)** 计算量太大，通常改用**迭代解法 (Iterative solution)**。

---

## 5. 动作价值 (Action Value / Q-Value)

除了评估“状态”好不好，我们更想知道“在某个状态下做一个**动作**”好不好。这就引出了 $q$ 值。

> [!abstract] 定义：动作价值函数 (Action-Value Function)
> 记为 $q_{\pi}(s, a)$。它表示在状态 $s$ 采取动作 $a$，并且**之后一直遵循策略 $\pi$** 所能获得的期望回报。
>
> $$q_{\pi}(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a]$$

### 5.1 $v$ 与 $q$ 的关系 (The Two Sides of the Same Coin)

这两个函数是可以互相转换的，就像硬币的两面。

1.  **从 $q$ 到 $v$** (状态价值是动作价值的平均):
    $$v_{\pi}(s) = \sum_{a} \pi(a|s) q_{\pi}(s, a)$$
    *(直觉：一个状态的价值，等于在这个状态下所有可能动作的价值，按照选动作的概率加权求和)*

2.  **从 $v$ 到 $q$** (动作价值是即时奖励 + 折扣后的未来状态价值):
    $$q_{\pi}(s, a) = \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v_{\pi}(s')$$

---

## 6. 总结 (Summary)

> [!summary] 核心知识点回顾
> 1.  **回报 ($G_t$)** 是随机的轨迹总收益；**价值 ($v_{\pi}$) **是回报的期望。
> 2.  **Bellman Equation** 是强化学习中最基础的方程，揭示了价值函数的递归结构：
>     $$v = \text{即时奖励} + \gamma \times \text{未来价值}$$
> 3.  **求解方法**：
>     * 解析解：$v = (I - \gamma P)^{-1}r$（仅适用于小规模状态空间）。
>     * 迭代解：$v_{k+1} = r + \gamma P v_k$（动态规划的基础）。
> 4.  **$v$ 值与 $q$ 值** 互为支撑，$v$ 是 $q$ 的期望，$q$ 是 $r + \gamma v'$。