---
date: 2026-01-28
tags:
  - 强化学习
---
# Lecture 2: Bellman Equation (贝尔曼方程)

## 1. 核心直觉 (Intuition)

在进入复杂的数学符号之前，我们先解决一个核心问题：**为什么我们需要计算“价值”？**

> [!example] 生活中的例子：股票投资 vs. 存钱
> 假设你现在有 10 万元（**State $S_t$**），你需要决定两个动作（**Action $A_t$**）：
> 1.  **存银行**：立刻获得很少的利息（**Immediate Reward $R_{t+1}$** 低），但明年你的本金基本不变，未来很稳。
> 2.  **买高风险股票**：立刻获得巨大的分红（**Immediate Reward $R_{t+1}$** 高），但明年可能本金归零（**Future State $S_{t+1}$** 变差）。
>
> 如果我们只看“眼前的奖励（Reward）”，可能会觉得股票好。但是，理性的决策必须包含**未来价值**的折现。
>
> **State Value (状态价值)** 就是在问：**“如果我身处这个状态（比如持有10万现金），并按照某套策略一直玩下去，我总共能（在未来）拿到多少好处？”**
>
> **Bellman Equation** 的核心直觉是 **Bootstrapping (自举/拔靴法)**：
> * 我今天的价值 = 我今天拿到的钱 + （打折后的）明天的价值。
> * $Value_{today} = Reward_{now} + \gamma \cdot Value_{tomorrow}$
>
> 这种**递归**的关系，就是贝尔曼方程的灵魂。

---

## 2. 基础定义：回报与状态价值 (Return & State Value)

### 2.1 回报 (Return)

强化学习的目标是最大化长期收益。我们需要定义一个核心概念：**Return (回报)**，通常记为 $G_t$。

> [!abstract] 定义：Discounted Return
> 从 $t$ 时刻开始，累积的折扣奖励之和：
> $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
> * $R_{t+1}$：执行动作后的即时奖励。
> * $\gamma \in [0, 1)$：**Discount Factor (折扣因子)**。$\gamma$ 越接近 0，表示越短视；$\gamma$ 越接近 1，表示越看重未来。

**为什么 $G_t$ 是随机变量？**
因为在这个过程中，以下环节都可能是随机的：
1.  **动作**：策略 $\pi(a|s)$ 可能是随机的。
2.  **奖励**：环境反馈 $p(r|s,a)$ 可能是随机的。
3.  **状态转移**：下一个状态 $p(s'|s,a)$ 可能是随机的。

### 2.2 状态价值函数 (State Value Function)

既然 $G_t$ 是随机的，我们无法直接优化它，所以我们要关注它的**期望 (Expectation)**。

> [!abstract] 定义：State Value Function
> 在状态 $s$ 下，遵循策略 $\pi$ 能获得的期望回报：
> $$v_{\pi}(s) = \mathbb{E}[G_t | S_t = s]$$
>

---

## 3. 核心推导：贝尔曼方程 (Bellman Equation Derivation)

这是本章的重头戏。我们需要推导出 $v_{\pi}(s)$ 的递归形式。

### 3.1 步骤一：回报的递归展开 (Recursive Property of Return)

首先，我们将 $G_t$ 展开并提取公因式 $\gamma$：

$$
\begin{aligned}
G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \\
&= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \\
&= R_{t+1} + \gamma G_{t+1}
\end{aligned}
$$

这说明：**现在的总回报 = 现在的奖励 + 打折后的下一时刻总回报**。

### 3.2 步骤二：代入价值函数定义 (Linearity of Expectation)

将上述关系代入 $v_{\pi}(s)$ 的定义中：

$$
\begin{aligned}
v_{\pi}(s) &= \mathbb{E}[G_t | S_t = s] \\
&= \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
&= \underbrace{\mathbb{E}[R_{t+1} | S_t = s]}_{\text{第一项：即时奖励期望}} + \gamma \underbrace{\mathbb{E}[G_{t+1} | S_t = s]}_{\text{第二项：未来回报期望}}
\end{aligned}
$$

这里利用了**期望的线性性质**：$\mathbb{E}[A+B] = \mathbb{E}[A] + \mathbb{E}[B]$。

### 3.3 步骤三：极详推导 (C-- Derivation)

我们需要分别计算这两项。这里需要用到**全期望公式**，即把所有可能的动作 $a$、奖励 $r$、下一状态 $s'$ 都考虑进来加权求和。

#### 第一项：即时奖励的期望 $\mathbb{E}[R_{t+1} | S_t = s]$

我们要计算在状态 $s$ 下能拿到的平均奖励。这个过程分两步随机：
1.  **选动作**：根据策略 $\pi(a|s)$ 选择动作 $a$。
2.  **给奖励**：环境根据 $p(r|s,a)$ 给反馈 $r$。

$$
\begin{aligned}
\mathbb{E}[R_{t+1} | S_t = s] &= \sum_{a} \pi(a|s) \cdot \mathbb{E}[R_{t+1} | S_t = s, A_t = a] \\
&= \sum_{a} \pi(a|s) \sum_{r} p(r|s,a) \cdot r
\end{aligned}
$$


#### 第二项：未来回报的期望 $\mathbb{E}[G_{t+1} | S_t = s]$

这是理解的难点。我们需要把 $G_{t+1}$ 关联到下一时刻的状态 $S_{t+1}$。

$$
\begin{aligned}
\mathbb{E}[G_{t+1} | S_t = s] &= \sum_{s'} \mathbb{E}[G_{t+1} | S_t = s, S_{t+1} = s'] \cdot p(s'|s) \\
&\text{（利用马尔可夫性：未来只与当前状态 } S_{t+1} \text{ 有关，与 } S_t \text{ 无关）} \\
&= \sum_{s'} \mathbb{E}[G_{t+1} | S_{t+1} = s'] \cdot p(s'|s) \\
&\text{（代入价值函数的定义 } v_{\pi}(s') = \mathbb{E}[G_{t+1} | S_{t+1} = s'] \text{）} \\
&= \sum_{s'} v_{\pi}(s') \cdot p(s'|s)
\end{aligned}
$$

其中 $p(s'|s)$ 是从 $s$ 转移到 $s'$ 的概率，它也需要对动作 $a$ 进行求和展开：
$$p(s'|s) = \sum_{a} \pi(a|s) p(s'|s,a)$$

将这两部分合起来，第二项变为：
$$\mathbb{E}[G_{t+1} | S_t = s] = \sum_{a} \pi(a|s) \sum_{s'} p(s'|s,a) v_{\pi}(s')$$


### 3.4 最终公式 (The Final Element-wise Equation)

将第一项和第二项合并，我们就得到了著名的 **Bellman Equation**：

> [!math] Bellman Equation (Element-wise)
> $$v_{\pi}(s) = \sum_{a} \pi(a|s) \left[ \sum_{r} p(r|s,a) r + \gamma \sum_{s'} p(s'|s,a) v_{\pi}(s') \right]$$
>

**直观解释：**
一个状态的价值 $v_{\pi}(s)$ = (在此状态下采取所有动作的平均即时奖励) + (打折后的所有可能到达的下一状态的价值平均)。

---

## 4. 矩阵形式与求解 (Matrix-Vector Form & Solution)

对于状态空间有限的问题（例如 Grid World），我们可以把每一个状态的贝尔曼方程写下来，组成一个线性方程组。

### 4.1 符号简化

为了简化书写，我们定义：
1.  **平均即时奖励** $r_{\pi}(s)$：
    $$r_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{r} p(r|s,a) r$$
2.  **状态转移概率** $p_{\pi}(s'|s)$：
    $$p_{\pi}(s'|s) = \sum_{a} \pi(a|s) p(s'|s,a)$$

此时贝尔曼方程简化为：
$$v_{\pi}(s) = r_{\pi}(s) + \gamma \sum_{s'} p_{\pi}(s'|s) v_{\pi}(s')$$

### 4.2 矩阵形式

假设有 $n$ 个状态，我们将 $v_{\pi}(s)$ 堆叠成向量 $v_{\pi}$，将 $r_{\pi}(s)$ 堆叠成向量 $r_{\pi}$，将 $p_{\pi}(s'|s)$ 组成矩阵 $P_{\pi}$（第 $i$ 行第 $j$ 列表示从状态 $i$ 跳到状态 $j$ 的概率）。

> [!math] Bellman Equation (Matrix Form)
> $$v_{\pi} = r_{\pi} + \gamma P_{\pi} v_{\pi}$$
>

### 4.3 求解方法

这是一个线性方程组，我们可以直接求解：

$$
\begin{aligned}
v_{\pi} - \gamma P_{\pi} v_{\pi} &= r_{\pi} \\
(I - \gamma P_{\pi}) v_{\pi} &= r_{\pi} \\
v_{\pi} &= (I - \gamma P_{\pi})^{-1} r_{\pi}
\end{aligned}
$$


> [!important] 注意
> * **直接求解 (Closed-form)**：需要计算矩阵求逆 $(I - \gamma P_{\pi})^{-1}$，复杂度为 $O(n^3)$，仅适用于状态数量较小的情况。
> * **迭代求解 (Iterative Solution)**：对于大状态空间，我们通常使用迭代法：
>     $$v_{k+1} = r_{\pi} + \gamma P_{\pi} v_{k}$$
>     当 $k \to \infty$ 时，序列会收敛到真实值 $v_{\pi}$。这正是后面章节 Value Iteration 的基础。

---

## 5. 动作价值函数 (Action Value Function)

有时候我们更关心“在状态 $s$ 采取特定动作 $a$”好不好，这就引出了 **Action Value (Q-Value)**。

> [!abstract] 定义：Action Value Function
> $$q_{\pi}(s,a) = \mathbb{E}[G_t | S_t = s, A_t = a]$$
>

### 5.1 V 与 Q 的关系 (Relationship)

这是一对像硬币两面一样的关系：

1.  **从 Q 到 V**（状态的价值 = 该状态下所有动作价值的加权平均）：
    $$v_{\pi}(s) = \sum_{a} \pi(a|s) q_{\pi}(s,a)$$
2.  **从 V 到 Q**（动作的价值 = 即时奖励 + 打折后的下一状态价值）：
    $$q_{\pi}(s,a) = \sum_{r} p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a) v_{\pi}(s')$$

这实际上把贝尔曼方程拆成了两步。在未来的 Q-Learning 中，我们会直接针对 $q_{\pi}$ 进行迭代。

---

## 6. 总结 (Summary)

1.  **目的**：我们通过 **Return** 来衡量策略的好坏。
2.  **工具**：**State Value** $v_{\pi}(s)$ 是回报的期望。
3.  **核心**：**Bellman Equation** 描述了状态之间的递归关系：
    * $v = r + \gamma P v$ （矩阵形式）
    * 这个方程不仅可以用来评估策略（Policy Evaluation），也是后续所有强化学习算法（如 Policy Iteration, Value Iteration）的基石。

> [!math] 核心公式速查
> $$v_{\pi}(s) = \sum_{a} \pi(a|s) \left[ \sum_{r} p(r|s,a) r + \gamma \sum_{s'} p(s'|s,a) v_{\pi}(s') \right]$$
> $$v_{\pi} = (I - \gamma P_{\pi})^{-1} r_{\pi}$$